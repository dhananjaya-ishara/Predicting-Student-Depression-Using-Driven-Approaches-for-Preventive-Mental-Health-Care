# -*- coding: utf-8 -*-
"""Predicting Student Depression Using Driven Approaches for Preventive Mental Health Care[ML-Driven Risk Stratification for Student Depression]-27595.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18V_bfjBWmL57wsgqvln4YE-SBF6pmhs5

## Import Related Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import scipy.stats as ss
import xgboost as xgb
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder
from google.colab import drive
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

"""## Import Dataset

"""

drive.mount('/content/drive')

# Replace 'your_dataset.csv' with the actual path to your dataset file
df = pd.read_csv('/content/drive/MyDrive/data set/student_depression_dataset.csv')
print("Dataset imported successfully!")
display(df.head())

"""##  Data Preprocessing & EDA

"""

# Check for null values in each column
null_counts = df.isnull().sum()

# Display the count of null values
print("Null values count for each column:")
print(null_counts)

df.head(5)

"""##  Value Counts , Replace Values & Data Type Conversion

"""

print(df.dtypes)

display(df['Sleep Duration'].value_counts())

# Replace old values with new values
df['Sleep Duration'].replace({
    "'More than 8 hours'": 9,
    "'5-6 hours'": 6,
    "'7-8 hours'": 8,
    "'Less than 5 hours'": 4,
    "Others": 0,
},inplace=True)

# Check the updated column
print(df['Sleep Duration'].value_counts())

display(df['Study Satisfaction'].value_counts())

df['Study Satisfaction'] = df['Study Satisfaction'].astype(int)

display(df['Study Satisfaction'].value_counts())

display(df['Job Satisfaction'].value_counts())

df['Job Satisfaction'] = df['Job Satisfaction'].astype(int)
display(df['Job Satisfaction'].value_counts())

display(df['Work Pressure'].value_counts())

df['Work Pressure'] = df['Work Pressure'].astype(int)

display(df['Work Pressure'].value_counts())

display(df['Academic Pressure'].value_counts())

# Replace old values with new values
df['Academic Pressure'] = df['Academic Pressure'].astype(int)
display(df['Academic Pressure'].value_counts())

display(df['Financial Stress'].value_counts())

df['Financial Stress'].replace({
    "?": 0.0,
},inplace=True)

display(df['Financial Stress'].value_counts())

df['Financial Stress'] = df['Financial Stress'].astype(float).fillna(0).astype(int)
display(df['Financial Stress'].value_counts())

display(df.head())

"""##  Find Outliers

"""

numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns


n_cols = len(numerical_cols)
n_rows = (n_cols + 2) // 3

plt.figure(figsize=(18, n_rows * 5))

for i, col in enumerate(numerical_cols, 1):
    plt.subplot(n_rows, 3, i)
    sns.boxplot(y=df[col])
    plt.title(f'Box plot of {col}')

plt.tight_layout()
plt.show()

"""##  Replace with column name and colum variable


"""

col = 'Age'

Q1 = df[col].quantile(0.25)
Q3 = df[col].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
print("Lower Bound:", lower_bound)
print("Upper Bound:", upper_bound)
print("number of count in outlier:", len(outliers))
print("Outlier rows based on 'Age' column:")
display(outliers)

col = 'CGPA'

Q1 = df[col].quantile(0.25)
Q3 = df[col].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
print("Lower Bound:", lower_bound)
print("Upper Bound:", upper_bound)
print("number of count in outlier:", len(outliers))
print("Outlier rows based on 'CGPA' column:")
display(outliers)

col = 'Work Pressure'

Q1 = df[col].quantile(0.25)
Q3 = df[col].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
print("Lower Bound:", lower_bound)
print("Upper Bound:", upper_bound)
print("number of count in outlier:", len(outliers))
print("Outlier rows based on 'Work Pressure' column:")
display(outliers)

col = 'Job Satisfaction'

Q1 = df[col].quantile(0.25)
Q3 = df[col].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
print("Lower Bound:", lower_bound)
print("Upper Bound:", upper_bound)
print("number of count in outlier:", len(outliers))
print("Outlier rows based on 'Job Satisfaction' column:")
display(outliers)

"""## Make New Dataset Variable

"""

cleaned_data=df

def data_count(col):
  display( cleaned_data[col].value_counts())
  print("--------------------------------------succes--------------------------------------")

cols = ['Gender', 'City', 'Profession', 'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?', ]
for coll in cols:
 data_count(coll)

"""##  Make New Dataset Variable

"""

df.hist(figsize=(15, 10))
plt.suptitle("Histograms of Numeric Columns")
plt.tight_layout()
plt.show()

print(cleaned_data.dtypes)

plt.figure(figsize=(10,7))
sns.barplot(
    data=cleaned_data,
    x='Profession',
    y='Academic Pressure', # Corrected column name
    estimator='mean',         # Show mean academic pressure per profession
    errorbar=None,            # Remove error bars (optional)
    palette='Blues',          # Color theme
    edgecolor='black'         # Outline for bars
)

plt.title("Academic Pressure by Profession", fontsize=13)
plt.xlabel("Profession")
plt.ylabel("Average Academic Pressure")
plt.xticks(rotation=90)
plt.grid(alpha=0.3, linestyle='--')
plt.tight_layout()
plt.show()

plt.figure(figsize=(7,5))
sns.barplot(
    data=df,
    x='Profession',
    y='Job Satisfaction',
    estimator='mean',
    errorbar=None,
    palette='Blues',
    edgecolor='black',
    hue='Job Satisfaction'
)

plt.title(" Job Satisfaction by Profession", fontsize=13)
plt.xlabel("Profession")
plt.ylabel("Average Job Satisfaction")
plt.xticks(rotation=90)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,6))
sns.barplot(
    data=cleaned_data,
    x='Profession',
    y='Work Pressure',
    estimator='mean',
    errorbar=None,
    palette='Blues_r',
    edgecolor='black',
    hue='Dietary Habits'
)

plt.title("Average Work Pressure by Profession", fontsize=14, fontweight='bold')
plt.xlabel("Profession", fontsize=12)
plt.ylabel("Average Work Pressure", fontsize=12)
plt.xticks(rotation=30, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8,5))
sns.barplot(
    data=cleaned_data,
    x='Profession',
    y='Sleep Duration',
    estimator='mean',
    errorbar='sd',
    palette='coolwarm',
    edgecolor='black',
)

plt.title("Average Sleep Duration by Profession", fontsize=14, fontweight='bold')
plt.xlabel("Profession", fontsize=12)
plt.ylabel("Average Sleep Duration (Hours)", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8,5))
sns.barplot(
    data=cleaned_data,
    x='Work Pressure',
    y='Job Satisfaction',
    estimator='mean',
    errorbar=None,
    palette='coolwarm',
    edgecolor='black',
    hue='Work Pressure'
)

plt.title("Work Pressure vs Job Satisfaction", fontsize=14)
plt.xlabel("Work Pressure Level")
plt.ylabel("Average Job Satisfaction")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8,5))
sns.barplot(
    data=cleaned_data,
    x='Work Pressure',
    y='Financial Stress' ,
    estimator='mean',
    errorbar=None,
    palette='coolwarm',
    edgecolor='black'
)

plt.title("Relationship Between Work Pressure and Financial Status", fontsize=13)
plt.xlabel("Work Pressure Level")
plt.ylabel("Finance Level")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8,5))
sns.barplot(
    data=cleaned_data,
    x='Academic Pressure',
    y='Sleep Duration',
    estimator='mean',
    errorbar=None,
    palette='coolwarm',
    edgecolor='black',
    hue='Profession'

)

plt.title("Academic Pressure vs Sleep Duration", fontsize=13)
plt.xlabel("Academic Pressure Level")
plt.ylabel("Sleep Duration (Hours)")
plt.legend(
    title="Profession",
    title_fontsize=10,
    fontsize=9,
    loc='center left',
    bbox_to_anchor=(1, 0.5),
    frameon=True
)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

plt.figure(figsize=(5,10))
sns.jointplot(
    data=cleaned_data,
    x='Academic Pressure',
    y='Work Pressure',
    kind='reg',
    color='green',
    height=7 ,

)
plt.suptitle("Academic Pressure vs Work Pressure", y=1.02)
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(
    data=cleaned_data,
    x='Dietary Habits',
    y='Academic Pressure',
    palette='pastel'
)

plt.title("Distribution of Academic Pressure by Dietary Habits", fontsize=13)
plt.xlabel("Dietary Habits")
plt.ylabel("Academic Pressure")
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# List of columns to plot against Dietary Habits
columns_to_plot = [
    'Sleep Duration',
    'Study Satisfaction',
    'Have you ever had suicidal thoughts ?',
    'Academic Pressure',
    'Work/Study Hours',
    'Age'
]

# Number of subplots
n_cols = 2
n_rows = 3

fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 18))
axes = axes.flatten()

for i, col in enumerate(columns_to_plot):
    # Check if the column exists in the dataframe before plotting
    if col in cleaned_data.columns:
        sns.boxplot(
            data=cleaned_data,
            x='Dietary Habits',
            y=col,
            hue='Gender',
            palette='Set2',
            ax=axes[i]
        )
        axes[i].set_title(f'Dietary Habits vs {col}', fontsize=12)
        axes[i].set_xlabel('Dietary Habits')
        axes[i].set_ylabel(col)
        axes[i].grid(axis='y', alpha=0.3)
    else:
        # If column does not exist, hide the subplot axis
        fig.delaxes(axes[i])
        print(f"Column '{col}' not found in the DataFrame.")


# Adjust layout and remove extra legend
plt.legend(
    title="Gender",
    title_fontsize=10,
    fontsize=9,
    loc='center left',
    bbox_to_anchor=(1, 0.5),
    frameon=True
)
plt.tight_layout()
plt.show()

# List of y-variables to plot against Dietary Habits
y_vars = [
    'Sleep Duration',
    'Study Satisfaction',
    'Have you ever had suicidal thoughts ?',
    'Academic Pressure',
    'Work/Study Hours',
    'Age'
]

# Set up the figure with 3 rows and 2 columns
fig, axes = plt.subplots(3, 2, figsize=(15, 15))
axes = axes.flatten()

# Loop through y variables and create plots
for i, y_var in enumerate(y_vars):
    # Check if the column exists in the dataframe before plotting
    if y_var in cleaned_data.columns:
        sns.boxplot(
            data=cleaned_data,
            x='Dietary Habits',
            y=y_var,
            hue='Gender',
            palette='Set2',
            ax=axes[i]


        )
        axes[i].set_title(f'Dietary Habits vs {y_var}', fontsize=12, fontweight='bold')
        axes[i].set_xlabel('Dietary Habits')
        axes[i].set_ylabel(y_var)
        axes[i].grid(alpha=0.3)
    else:
        # If column does not exist, hide the subplot axis
        fig.delaxes(axes[i])
        print(f"Column '{y_var}' not found in the DataFrame.")

# Adjust layout and legend
plt.tight_layout()
# Add a single legend for the hue across all subplots
handles, labels = axes[0].get_legend_handles_labels()
fig.legend(handles, labels, title='Gender', bbox_to_anchor=(1.05, 1), loc='upper left')

# Remove individual legends from subplots
for ax in axes.flatten():
    if ax.get_legend():
        ax.get_legend().remove()

plt.legend(
    title="Gender",
    title_fontsize=10,
    fontsize=9,
    loc='center left',
    bbox_to_anchor=(1, 0.5),
    frameon=True
)
plt.show()

plt.figure(figsize=(8,5))
plt.style.use('fivethirtyeight')
sns.scatterplot(
    data=cleaned_data,
    x='Work/Study Hours',
    y='Age',
    alpha=0.6
)
plt.legend()
plt.title(' Study Time vs Age')
plt.tight_layout()
plt.show()

# List of columns to plot vs 'Profession'
columns_to_plot = [
    'Sleep Duration',
    'Study Satisfaction',
    'Have you ever had suicidal thoughts ?',
    'Academic Pressure',
    'Work Pressure',
    'Work/Study Hours',
    'Job Satisfaction',
    'Degree',
    'Financial Stress',
    'Age'
]

# Set up the subplot grid
n_cols = 1
n_rows = (len(columns_to_plot) + 1) // n_cols
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 10))
axes = axes.flatten()

# Loop through columns and plot barplots
for i, col in enumerate(columns_to_plot):
    sns.barplot(
        data=df,
        x='Profession',
        y=col,
        hue=col,
        ax=axes[i],
        errorbar=None,
        palette='Set2',
        edgecolor='black'
    )
    axes[i].set_title(f'{col} vs Profession', fontsize=12)
    axes[i].set_xlabel('Profession')
    axes[i].set_ylabel(col)
    axes[i].tick_params(axis='x', rotation=45, colors='black')
    axes[i].tick_params(axis='y', colors='black')


# Remove any empty subplots
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])
plt.legend(
    title="col",
    title_fontsize=10,
    fontsize=9,
    loc='center left',
    bbox_to_anchor=(1, 0.5),
    frameon=True
)
plt.tight_layout()
plt.show()

columns_to_plot = [
    'Gender', 'Age', 'City', 'Profession', 'Academic Pressure', 'Work Pressure', 'Study Satisfaction', 'Job Satisfaction', 'Sleep Duration',
    'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?',
    'Work/Study Hours', 'Financial Stress', 'Family History of Mental Illness'
]
# Number of subplots
n_cols = 3
n_rows = (len(columns_to_plot) + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 70))
axes = axes.flatten()

for i, col in enumerate(columns_to_plot):
    sns.countplot(
        data=cleaned_data,
        x='Depression',
        hue=col,
        palette='Set2',
        ax=axes[i]
    )
    axes[i].set_title(f'Depression vs {col}', fontsize=12)
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].legend(loc='upper right', fontsize=8)
    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

# Remove empty subplots
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])
plt.legend(
    title="col",
    title_fontsize=10,
    fontsize=9,
    loc='center left',
    bbox_to_anchor=(1, 0.5),
    frameon=True
)
plt.tight_layout()
plt.show()

"""## Categorical Variable Correlation Function

"""

def correlations_variable(x, y):
    # Build the confusion matrix
    confusion_matrix = pd.crosstab(x, y)

    # Perform Chi-squared test
    chi2 = ss.chi2_contingency(confusion_matrix)[0]

    # Number of observations
    n = confusion_matrix.sum().sum()

    # Calculate phi-squared
    phi2 = chi2 / n

    # Rows and columns of the matrix
    r, k = confusion_matrix.shape

    # Correction for bias
    phi2_corr = max(0, phi2 - ((k - 1)*(r - 1)) / (n - 1))
    r_corr = r - ((r - 1)**2) / (n - 1)
    k_corr = k - ((k - 1)**2) / (n - 1)

    # Calculate Cram√©r's V
    return np.sqrt(phi2_corr / min((k_corr - 1), (r_corr - 1)))

# Correlation between all categorical features
categorical_cols = ['Gender','City','Profession', 'Dietary Habits', 'Degree','Have you ever had suicidal thoughts ?','Family History of Mental Illness']
for col1 in categorical_cols:
    for col2 in categorical_cols:
        if col1 != col2:
            print(f"{col1} vs {col2}: {correlations_variable(cleaned_data[col1], cleaned_data[col2]):.3f}")

"""## Encode Categorical Features & Prepare Dataset

"""

# Identify categorical columns from the current DataFrame
categorical_cols = ['Gender','City','Profession', 'Dietary Habits', 'Degree','Have you ever had suicidal thoughts ?','Family History of Mental Illness']

target_col = 'Depression'

# Separate features and target
features_cleaned_data = cleaned_data.drop(columns=[target_col])
target_cleaned_data = cleaned_data[target_col]

nominal_cols = features_cleaned_data.select_dtypes(include=['object']).columns.tolist()

# Apply One-Hot Encoding to nominal columns
onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_nominal = onehot_encoder.fit_transform(features_cleaned_data[nominal_cols])
encoded_nominal_cleaned_data = pd.DataFrame(encoded_nominal, columns=onehot_encoder.get_feature_names_out(nominal_cols), index=features_cleaned_data.index)

# Identify numerical columns
numerical_cols = features_cleaned_data.select_dtypes(include=np.number).columns.tolist()

numerical_cleaned_data = features_cleaned_data[numerical_cols]

cleaned_data_processed = pd.concat([numerical_cleaned_data, encoded_nominal_cleaned_data, target_cleaned_data], axis=1)

display(cleaned_data_processed.head())

categorical_cols = ['Gender','City','Profession', 'Dietary Habits', 'Degree','Have you ever had suicidal thoughts ?','Family History of Mental Illness']
target_col = 'Depression'

features_cleaned_data = cleaned_data.drop(columns=[target_col])
target_cleaned_data = cleaned_data[target_col]

nominal_cols = features_cleaned_data.select_dtypes(include=['object']).columns.tolist()

onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_nominal = onehot_encoder.fit_transform(features_cleaned_data[nominal_cols])
encoded_nominal_cleaned_data = pd.DataFrame(encoded_nominal, columns=onehot_encoder.get_feature_names_out(nominal_cols), index=features_cleaned_data.index)

numerical_cols = features_cleaned_data.select_dtypes(include=np.number).columns.tolist()

numerical_cleaned_data = features_cleaned_data[numerical_cols]

processed = pd.concat([numerical_cleaned_data, encoded_nominal_cleaned_data, target_cleaned_data], axis=1)

display(cleaned_data_processed.head())

"""## Correlation Heatmap: Numerical Features & Target

"""

numerical_cols = numerical_cleaned_data.columns.tolist()
target_col = 'Depression'
cols_to_use = numerical_cols + [target_col]

df_numerical_target = cleaned_data_processed[cols_to_use]

plt.figure(figsize=(10, 10))

corr_matrix = df_numerical_target.corr()
sns.heatmap(
    corr_matrix,
    annot=True,
    cmap='coolwarm',
    fmt=".2f",
    linewidths=0.5,
    cbar_kws={'label': 'Correlation'}
)

plt.title('Correlation Matrix of Numerical Features and Depression', fontsize=14, fontweight='bold')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

processed

"""## Standardization & Normalization of Numerical Features

"""

numerical_cols = cleaned_data_processed.select_dtypes(include=np.number).columns.tolist()

if 'Depression' in numerical_cols:
    numerical_cols.remove('Depression')

scaler_standard = StandardScaler()
cleaned_data_standardized = cleaned_data_processed.copy()
cleaned_data_standardized[numerical_cols] = scaler_standard.fit_transform(cleaned_data_standardized[numerical_cols])

print("DataFrame after Standardization (first 5 rows):")
display(cleaned_data_standardized.head())

scaler_minmax = MinMaxScaler()
cleaned_data_normalized = cleaned_data_processed.copy()
cleaned_data_normalized[numerical_cols] = scaler_minmax.fit_transform(cleaned_data_normalized[numerical_cols])

print("\nDataFrame after Normalization (Min-Max Scaling) (first 5 rows):")
display(cleaned_data_normalized.head())

"""##  Train-Test Split

"""

X = cleaned_data_normalized.drop(columns='Depression')
y = cleaned_data_normalized['Depression']

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

"""## Create and Test Model

##Define Models
"""

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(kernel='rbf', probability=True, random_state=42),
    "Extra Trees": ExtraTreesClassifier(n_estimators=100, random_state=42),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),
    "LightGBM": lgb.LGBMClassifier(random_state=42)
}

results = {}


for name, model in models.items():
    print(f"\n===== Training {name} =====")


    model.fit(X_train, y_train)


    y_train_pred = model.predict(X_train)
    train_metrics = {
        "Accuracy": accuracy_score(y_train, y_train_pred),
        "Precision": precision_score(y_train, y_train_pred, zero_division=0),
        "Recall": recall_score(y_train, y_train_pred, zero_division=0),
        "F1 Score": f1_score(y_train, y_train_pred, zero_division=0),
        "Confusion Matrix": confusion_matrix(y_train, y_train_pred).tolist()
    }

    try:
        if hasattr(model, "predict_proba"):
            y_train_prob = model.predict_proba(X_train)[:, 1]
            train_metrics["ROC AUC"] = roc_auc_score(y_train, y_train_prob)
        else:
            train_metrics["ROC AUC"] = "N/A"
    except Exception as e:
        train_metrics["ROC AUC"] = f"Error: {e}"


    y_test_pred = model.predict(X_test)
    test_metrics = {
        "Accuracy": accuracy_score(y_test, y_test_pred),
        "Precision": precision_score(y_test, y_test_pred, zero_division=0),
        "Recall": recall_score(y_test, y_test_pred, zero_division=0),
        "F1 Score": f1_score(y_test, y_test_pred, zero_division=0),
        "Confusion Matrix": confusion_matrix(y_test, y_test_pred).tolist()
    }


    try:
        if hasattr(model, "predict_proba"):
            y_test_prob = model.predict_proba(X_test)[:, 1]
            test_metrics["ROC AUC"] = roc_auc_score(y_test, y_test_prob)
        else:
            test_metrics["ROC AUC"] = "N/A"
    except Exception as e:
        test_metrics["ROC AUC"] = f"Error: {e}"


    results[name] = {
        "Train Metrics": train_metrics,
        "Test Metrics": test_metrics
    }

    print(f"{name} - Train Accuracy: {train_metrics['Accuracy']:.4f}, Test Accuracy: {test_metrics['Accuracy']:.4f}")
    print("-" * 50)


flattened_results = []

for model_name, metrics in results.items():
    row = {"Model": model_name}
    for prefix, metric_dict in metrics.items():
        for metric_name, value in metric_dict.items():
            row[f"{prefix} {metric_name}"] = value
    flattened_results.append(row)

results_cleaned_data = pd.DataFrame(flattened_results)
display(results_cleaned_data)

"""## Model Evaluation & Best Model Selection

"""

# Flatten the nested dictionary into a DataFrame
flattened_results = []

for model_name, metrics in results.items():
    row = {"Model": model_name}
    for prefix, metric_dict in metrics.items():
        for metric_name, value in metric_dict.items():
            row[f"{prefix} {metric_name}"] = value
    flattened_results.append(row)

results_df = pd.DataFrame(flattened_results).set_index("Model")

# Identify the model with the highest Test Accuracy
best_model_name = results_df["Test Metrics Accuracy"].astype(float).idxmax()
best_model_score = results_df.loc[best_model_name, "Test Metrics Accuracy"]

# Print best model summary
print("===========================================")
print(f" Best Model: {best_model_name}")
print(f" Test Accuracy: {best_model_score:.4f}")
print("===========================================")

# Show detailed performance metrics
print("\nDetailed performance of the best model:\n")
display(results_df.loc[[best_model_name]])

"""##  Test with User Inputs
### Selected Models: Logistic Regression, SVM

"""

# Example user input (replace values with actual user data)
user_input_raw = {
    'Gender': 'Male',
    'Age': 22.0,
    'City': 'Visakhapatnam',
    'Profession': 'Student',
    'Academic Pressure': 3.0,
    'Work Pressure': 0.0,
    'CGPA': 3.5,
    'Study Satisfaction': 4.0,
    'Job Satisfaction': 0.0,
    'Sleep Duration': 6,
    'Dietary Habits': 'Healthy',
    'Degree': 'B.Pharm',
    'Have you ever had suicidal thoughts ?': 'No',
    'Work/Study Hours': 5.0,
    'Financial Stress': 2,
    'Family History of Mental Illness': 'No'
}

# Convert to DataFrame
user_df_raw = pd.DataFrame([user_input_raw])

#  Separate categorical and numerical columns
categorical_cols_user = user_df_raw.select_dtypes(include='object').columns.tolist()
numerical_cols_user = user_df_raw.select_dtypes(include=np.number).columns.tolist()

# Apply One-Hot Encoding to nominal categorical columns
user_encoded_nominal = onehot_encoder.transform(user_df_raw[categorical_cols_user])
user_encoded_nominal_df = pd.DataFrame(user_encoded_nominal, columns=onehot_encoder.get_feature_names_out(categorical_cols_user), index=user_df_raw.index)

#  Select numerical columns
user_numerical_df = user_df_raw[numerical_cols_user]

#  Concatenate numerical and encoded nominal features
user_df_processed = pd.concat([user_numerical_df, user_encoded_nominal_df], axis=1)

#  Ensure all columns from training data are present, fill missing with 0
train_cols = X_train.columns
user_df_processed = user_df_processed.reindex(columns=train_cols, fill_value=0)

#  Apply Min-Max Scaling to numerical columns
numerical_cols_user_processed = user_df_processed.select_dtypes(include=np.number).columns.tolist()

user_df_scaled = user_df_processed.copy()
user_df_scaled[numerical_cols_user_processed] = scaler_minmax.transform(user_df_scaled[numerical_cols_user_processed])

# Display the preprocessed user input
print("Preprocessed user input:")
display(user_df_scaled)


print("\nPredictions based on preprocessed user input:")
for name, model in models.items():

    prediction = model.predict(user_df_scaled)
    if hasattr(model, "predict_proba"):
        probability = model.predict_proba(user_df_scaled)[:, 1]
        print(f"{name} Prediction: {prediction[0]}, Probability: {probability[0]:.2f}")
    else:
        print(f"{name} Prediction: {prediction[0]}")